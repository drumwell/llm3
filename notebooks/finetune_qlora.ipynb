{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# BMW E30 M3 Service Manual - QLoRA Finetuning\n\nThis notebook finetunes **Llama-3.1-8B-Instruct** on BMW service manual data using QLoRA.\n\n## Dataset Statistics\n- Training: 2,510 examples (all service manual data)\n- Validation: 248 examples (synthetic)\n- Tasks: SPEC, PROCEDURE, EXPLANATION, WIRING, TROUBLESHOOTING\n\n## Requirements\n- GPU: T4 (Colab free), A100 (Colab Pro), or better\n- VRAM: ~16-20 GB (8B model with QLoRA)\n- Time: ~2-3 hours on T4, ~45-60 min on A100"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 1: Setup & Authentication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install -q accelerate peft bitsandbytes transformers trl datasets wandb\n",
    "\n",
    "# Authenticate with HuggingFace\n",
    "from huggingface_hub import notebook_login\n",
    "notebook_login()  # Enter your HF token when prompted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Cell 2: Mount Google Drive (for dataset upload)\n\n**Before running**: Upload the following to Google Drive at `/content/drive/MyDrive/bmw_finetuning/data/`:\n- `hf_train_autotrain.jsonl` (2,510 examples)\n- `hf_val_synthetic.jsonl` (248 examples)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from google.colab import drive\ndrive.mount('/content/drive')\n\n# Verify files exist\nimport os\nbase_path = '/content/drive/MyDrive/bmw_finetuning'\nrequired_files = [\n    f'{base_path}/data/hf_train_autotrain.jsonl',\n    f'{base_path}/data/hf_val_synthetic.jsonl'\n]\n\nprint(\"Checking required files...\")\nfor file in required_files:\n    if os.path.exists(file):\n        print(f\"‚úÖ {file}\")\n    else:\n        print(f\"‚ùå MISSING: {file}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Cell 3: Load Datasets\n\nThe data is already in flat text format: `{\"text\": \"User: [TASK] Q\\nAssistant: A\"}`"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import json\nfrom datasets import Dataset\n\n# Load datasets (already in flat text format)\ndef load_jsonl(path):\n    data = []\n    with open(path) as f:\n        for line in f:\n            data.append(json.loads(line))\n    return Dataset.from_list(data)\n\ntrain_dataset = load_jsonl('/content/drive/MyDrive/bmw_finetuning/data/hf_train_autotrain.jsonl')\nval_dataset = load_jsonl('/content/drive/MyDrive/bmw_finetuning/data/hf_val_synthetic.jsonl')\n\nprint(f\"‚úÖ Loaded datasets - Train: {len(train_dataset)}, Val: {len(val_dataset)}\")\n\n# Show sample\nprint(\"\\nüìù Sample training example:\")\nsample = train_dataset[0]\nprint(f\"Text: {sample['text'][:200]}...\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Cell 4: Load Llama 3.1 8B with QLoRA"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\nfrom peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\nimport torch\n\nmodel_name = \"meta-llama/Llama-3.1-8B-Instruct\"\nprint(f\"üîÑ Loading model: {model_name}\")\n\n# 4-bit quantization configuration\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_compute_dtype=torch.bfloat16,\n    bnb_4bit_use_double_quant=True\n)\n\n# Load base model in 4-bit\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    quantization_config=bnb_config,\n    device_map=\"auto\",\n    trust_remote_code=True,\n    torch_dtype=torch.bfloat16\n)\n\n# Load tokenizer\ntokenizer = AutoTokenizer.from_pretrained(\n    model_name, \n    trust_remote_code=True\n)\n\n# Set padding token (Llama doesn't have one by default)\nif tokenizer.pad_token is None:\n    tokenizer.pad_token = tokenizer.eos_token\n    tokenizer.pad_token_id = tokenizer.eos_token_id\n\nprint(f\"‚úÖ Tokenizer loaded - Vocab size: {len(tokenizer)}\")\n\n# Prepare model for k-bit training\nmodel.config.use_cache = False  # Disable cache for training\nmodel = prepare_model_for_kbit_training(model)\n\n# Configure QLoRA\nlora_config = LoraConfig(\n    r=16,\n    lora_alpha=32,\n    target_modules=[\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\"],\n    lora_dropout=0.05,\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\"\n)\n\n# Apply LoRA adapters\nmodel = get_peft_model(model, lora_config)\n\n# Print trainable parameters\nprint(\"\\nüìä Trainable parameters:\")\nmodel.print_trainable_parameters()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Cell 5: Configure Training Arguments\n\nData is already formatted, no need for chat template formatting."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Data is already in flat text format {\"text\": \"User: ...\\nAssistant: ...\"}\n# No formatting needed - ready for training!\n\nprint(\"‚úÖ Data already formatted in flat text format\")\nprint(f\"\\nüìù Sample:\\n{train_dataset[0]['text'][:300]}...\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Cell 6: Configure Training Arguments"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from transformers import TrainingArguments\n\noutput_dir = \"./bmw_e30_qlora_results\"\n\ntraining_args = TrainingArguments(\n    output_dir=output_dir,\n    num_train_epochs=3,\n    per_device_train_batch_size=4,\n    per_device_eval_batch_size=4,\n    gradient_accumulation_steps=4,  # Effective batch = 16\n    gradient_checkpointing=True,  # Saves memory\n    learning_rate=2e-4,\n    warmup_ratio=0.1,\n    weight_decay=0.01,\n    logging_steps=10,\n    logging_dir=f\"{output_dir}/logs\",\n    save_strategy=\"epoch\",\n    eval_strategy=\"epoch\",\n    eval_steps=None,\n    save_total_limit=2,  # Only keep 2 best checkpoints\n    fp16=True,  # Mixed precision training\n    bf16=False,\n    optim=\"paged_adamw_8bit\",\n    report_to=\"none\",  # Change to \"wandb\" if you want tracking\n    push_to_hub=False,\n    max_grad_norm=0.3,  # Gradient clipping\n    lr_scheduler_type=\"cosine\"\n)\n\neffective_batch = training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps\ntotal_steps = (len(train_dataset) // effective_batch) * training_args.num_train_epochs\n\nprint(\"‚úÖ Training arguments configured\")\nprint(f\"üìä Effective batch size: {effective_batch}\")\nprint(f\"‚è±Ô∏è  Total training steps: ~{total_steps}\")\nprint(f\"üî• Warmup steps: ~{int(total_steps * training_args.warmup_ratio)}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Cell 7: Initialize Trainer and Start Training\n\n**Expected training time**:\n- T4 (Colab free): ~2-3 hours\n- A100: ~45-60 minutes"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from trl import SFTTrainer\n\n# Formatting function to extract text from dataset\ndef formatting_func(example):\n    return example[\"text\"]\n\n# Initialize trainer\ntrainer = SFTTrainer(\n    model=model,\n    train_dataset=train_dataset,\n    eval_dataset=val_dataset,\n    args=training_args,\n    formatting_func=formatting_func,\n    max_seq_length=512\n)\n\nprint(\"üöÄ Starting training...\")\nprint(f\"‚è±Ô∏è  Estimated time: 2-3 hours on T4, ~1 hour on A100\\n\")\n\n# Train the model\ntrainer.train()\n\nprint(\"\\n‚úÖ Training complete!\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Cell 8: Evaluate on Validation Set"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate\n",
    "print(\"üìä Evaluating on validation set...\")\n",
    "eval_results = trainer.evaluate()\n",
    "\n",
    "print(\"\\nüìà Evaluation Results:\")\n",
    "for key, value in eval_results.items():\n",
    "    print(f\"  {key}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Cell 9: Save Model Locally"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the fine-tuned model\n",
    "save_dir = \"./bmw_e30_m3_service_manual\"\n",
    "\n",
    "print(f\"üíæ Saving model to {save_dir}...\")\n",
    "trainer.save_model(save_dir)\n",
    "tokenizer.save_pretrained(save_dir)\n",
    "\n",
    "print(\"‚úÖ Model saved locally\")\n",
    "\n",
    "# Also save to Google Drive for persistence\n",
    "import shutil\n",
    "drive_save_dir = '/content/drive/MyDrive/llm3/models/bmw_e30_m3_service_manual'\n",
    "print(f\"\\nüíæ Copying to Google Drive: {drive_save_dir}...\")\n",
    "shutil.copytree(save_dir, drive_save_dir, dirs_exist_ok=True)\n",
    "print(\"‚úÖ Model saved to Google Drive\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Cell 10: Test Inference (Quick Validation)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick inference test\n",
    "def test_model(prompt):\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    text = tokenizer.apply_chat_template(\n",
    "        messages, \n",
    "        tokenize=False, \n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "    \n",
    "    inputs = tokenizer(text, return_tensors=\"pt\").to(model.device)\n",
    "    \n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=100,\n",
    "        temperature=0.7,\n",
    "        do_sample=True,\n",
    "        top_p=0.9,\n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "    \n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    # Extract just the assistant's response\n",
    "    response = response.split(\"assistant\\n\")[-1] if \"assistant\" in response else response\n",
    "    return response\n",
    "\n",
    "# Test cases\n",
    "print(\"üß™ Testing model with sample queries:\\n\")\n",
    "\n",
    "test_queries = [\n",
    "    \"[SPEC] What is the torque for cylinder head bolts?\",\n",
    "    \"[PROCEDURE] How do you adjust valve clearance?\",\n",
    "    \"[EXPLANATION] Explain the Motronic control unit operation\"\n",
    "]\n",
    "\n",
    "for query in test_queries:\n",
    "    print(f\"‚ùì Query: {query}\")\n",
    "    print(f\"üí¨ Response: {test_model(query)}\\n\")\n",
    "    print(\"-\" * 80 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Cell 11: Push to HuggingFace Hub (Optional)\n\n**Note**: Change `hub_model_id` to your HuggingFace username before running!"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Push to HuggingFace Hub\n",
    "hub_model_id = \"your-username/llm3\"  # ‚ö†Ô∏è Change to your username!\n",
    "\n",
    "print(f\"üöÄ Pushing model to HuggingFace Hub: {hub_model_id}\")\n",
    "print(\"‚è±Ô∏è  This may take a few minutes...\\n\")\n",
    "\n",
    "model.push_to_hub(hub_model_id, use_auth_token=True)\n",
    "tokenizer.push_to_hub(hub_model_id, use_auth_token=True)\n",
    "\n",
    "print(f\"‚úÖ Model successfully pushed!\")\n",
    "print(f\"üîó View at: https://huggingface.co/{hub_model_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Cell 12: Load from Hub (Test Deployment)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Test loading from HuggingFace Hub\nfrom peft import PeftModel, PeftConfig\n\nprint(f\"üîÑ Loading model from Hub: {hub_model_id}\")\n\n# Load config\npeft_config = PeftConfig.from_pretrained(hub_model_id)\n\n# Load base model\nbase_model = AutoModelForCausalLM.from_pretrained(\n    peft_config.base_model_name_or_path,\n    device_map=\"auto\",\n    torch_dtype=torch.bfloat16\n)\n\n# Load adapter\nmodel_from_hub = PeftModel.from_pretrained(base_model, hub_model_id)\n\nprint(\"‚úÖ Model loaded from Hub successfully!\")\n\n# Quick test\ntest_prompt = \"[SPEC] What is the engine displacement?\"\nprint(f\"\\nüß™ Test query: {test_prompt}\")\ninputs = tokenizer(test_prompt, return_tensors=\"pt\").to(model_from_hub.device)\noutputs = model_from_hub.generate(**inputs, max_new_tokens=50, temperature=0.7, do_sample=True)\nresponse = tokenizer.decode(outputs[0], skip_special_tokens=True)\nprint(f\"üí¨ Response: {response}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Next Steps\n\n1. **Evaluate thoroughly**: Test on diverse queries from the validation set\n2. **Monitor for overfitting**: Check if train/eval loss diverged\n3. **Adjust hyperparameters** if needed:\n   - Increase LoRA rank (16 ‚Üí 32) if underfitting\n   - Increase dropout (0.05 ‚Üí 0.1) if overfitting\n   - Train for more epochs if loss still decreasing\n4. **Deploy**: Use the model from HuggingFace Hub for inference\n5. **Collect feedback**: Test with real BMW technicians if possible\n\n## Resources\n\n- [QLoRA Paper](https://arxiv.org/abs/2305.14314)\n- [Llama 3.1 Model Card](https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct)\n- [PEFT Documentation](https://huggingface.co/docs/peft)\n- [TRL Documentation](https://huggingface.co/docs/trl)"
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}